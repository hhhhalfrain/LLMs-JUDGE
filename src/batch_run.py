# -*- coding: utf-8 -*-
"""
batch_run.py  (å®éªŒçº§æ–­ç‚¹ç»­è·‘ + å¤šç«¯ç‚¹è´Ÿè½½å‡è¡¡ + STAGE2 ç»“æœå¤ç”¨)
------------------------------------------------------------
æ–­ç‚¹ç»­è·‘ç­–ç•¥ï¼š
- è‹¥ outputs/<run_id>/_DONE.json å­˜åœ¨ï¼šè·³è¿‡è¯¥å®éªŒ
- è‹¥ outputs/<run_id>/ å­˜åœ¨ä½†æ²¡æœ‰ _DONE.jsonï¼šåˆ æ‰æ•´ä¸ªç›®å½•åé‡è·‘
"""

from __future__ import annotations

import json
import os
import shutil
import time
from pathlib import Path
from types import SimpleNamespace
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Any, Dict, List, Tuple

from dotenv import load_dotenv

from src.runner import (
    setup_logger,
    ThreadLocalLLM,
    AdaptiveConcurrencyController,
    evaluate_single_book,
)


# ============================================================
# âœ… ç¡¬ç¼–ç é…ç½®åŒº
# ============================================================
load_dotenv()
PROJECT_ROOT = Path(__file__).resolve().parents[1]

BOOKS_MERGED_JSON = PROJECT_ROOT / "data" / "books" / "merged_books_fixed.json"
PERSONAS_JSON = PROJECT_ROOT / "data" / "personas_sample.json"

# ç»­è·‘å…³é”®ï¼šå›ºå®šè¿™ä¸ªç›®å½•åï¼Œé‡å¤è¿è¡Œä¼šåœ¨åŒä¸€æ‰¹æ¬¡ä¸Šè·³è¿‡/åˆ é™¤é‡è·‘
BATCH_ID = "4agents exp1"
BATCH_ROOT = PROJECT_ROOT / "runs" / "batch" / BATCH_ID
OUTPUTS_ROOT = BATCH_ROOT / "outputs"
BASE_EVAL_ROOT = BATCH_ROOT / "base_eval"  # å…¨å±€åŸºçº¿è¯„æµ‹ç¼“å­˜ï¼ˆè·¨ experiment å¤ç”¨ï¼‰

# å¤šä¸ªåƒé—®ç«¯ç‚¹ï¼ˆä½ éœ€è¦å¡«æˆçœŸå®çš„ 6 ä¸ª base_urlï¼‰
QWEN_BASE_URLS = [
    "http://10.130.71.2:30071/v1",
    "http://10.130.71.2:30921/v1",
    "http://10.130.71.2:30781/v1",
    "http://10.130.71.2:30116/v1",
    "http://10.130.71.2:30810/v1",
]

# é¡¶å±‚â€œæ¯ä¸ªç«¯ç‚¹ä¸€ä¸ªå¤§å®éªŒçº¿ç¨‹â€
MAX_EXPERIMENT_WORKERS = max(1, len(QWEN_BASE_URLS))
START_STAGGER_SEC = 1.0

# âœ… ç« èŠ‚åˆå¹¶ batchsize
CHAPTER_BATCH_SIZE = 1   # <= 1 è¡¨ç¤ºä¸åˆå¹¶ï¼ˆä¿æŒåŸæ ·ï¼‰

# LLM é…ç½®ï¼ˆåƒé—® 32Bï¼ŒDashScope compatibleï¼‰
LLM_MODEL = "qwen-32b"
API_KEY_ENV = "QWEN_API_KEY"

LLM_TEMPERATURE = 0.6
LLM_TOP_P = 0.9
LLM_MAX_TOKENS = 10000
LLM_TIMEOUT_SEC = 1500

# è¯·æ±‚çº§é‡è¯•
RETRY_MAX_ATTEMPTS = 1000
RETRY_BASE_SLEEP_SEC = 1.0
RETRY_MAX_SLEEP_SEC = 30.0
RETRY_JITTER = 0.2
FAIL_FAST = True

# runner å†…éƒ¨ï¼šåŒä¸€æœ¬ä¹¦ agent å¹¶å‘çš„ä¸Šé™ï¼ˆè‡ªé€‚åº”æ§åˆ¶ä¼šåœ¨è¿™ä¸ªèŒƒå›´å†…æµ®åŠ¨ï¼‰
PER_BOOK_AGENT_WORKERS = 4


# -----------------------------
# âœ… ç½‘æ ¼ç»´åº¦ï¼ˆå…¨éƒ¨å¯æ‰«ï¼‰
# -----------------------------
METHODS = ["aggregation", "incremental", "summary_based"]
USE_PERSONA_OPTS = [False,True]
USE_DISCUSSION_OPTS = [False,True]
USE_INTEREST_FILTER_OPTS = [True]

DISCUSSION_ROUNDS_OPTS = [1,2,4]
DISCUSSION_WINDOW_OPTS = [8]
N_AGENTS_OPTS = [4]
SCORE_DECIMALS_OPTS = [1]
DISCUSSION_AFFECTS_SCORE_OPTS = [True]


# ============================================================
# å·¥å…·å‡½æ•°
# ============================================================
def safe_mkdir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)


def read_json(path: Path) -> Any:
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def atomic_write_json(path: Path, obj: Dict[str, Any]) -> None:
    safe_mkdir(path.parent)
    tmp = path.with_suffix(path.suffix + ".tmp")
    with open(tmp, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)
    os.replace(tmp, path)


def sanitize_name(s: str, max_len: int = 180) -> str:
    bad = '<>:"/\\|?*'
    s = (s or "").strip()
    for ch in bad:
        s = s.replace(ch, "_")
    s = s.strip().strip(".")
    return s[:max_len] if len(s) > max_len else s


def _safe_int(x: Any, default: int = 0) -> int:
    try:
        return int(x)
    except Exception:
        return default


def batch_merge_chapters(book_record: Dict[str, Any], batch_size: int, inplace: bool = True) -> Dict[str, Any]:
    """
    å°† book_record["chapter"] æˆ– ["chapters"] æŒ‰ batch_size åˆå¹¶ã€‚
    - inplace=Trueï¼šç›´æ¥ä¿®æ”¹åŸ book_record
    - å…¼å®¹å­—æ®µåï¼šchapter / chapters
    """
    if not isinstance(book_record, dict):
        return book_record
    if batch_size is None or int(batch_size) <= 1:
        return book_record

    chap_key = None
    if isinstance(book_record.get("chapters", None), list):
        chap_key = "chapters"
    elif isinstance(book_record.get("chapter", None), list):
        chap_key = "chapter"
    else:
        return book_record

    if book_record.get("_chapter_batched", False) and book_record.get("_chapter_batch_size", None) == int(batch_size):
        return book_record

    chapters = book_record.get(chap_key, [])
    if not chapters:
        return book_record

    def chap_key_fn(c: Any, idx: int) -> Tuple[int, int]:
        if isinstance(c, dict) and ("Number" in c):
            return (_safe_int(c.get("Number", idx + 1), idx + 1), idx)
        return (idx + 1, idx)

    chapters_sorted = sorted(
        [(c, i) for i, c in enumerate(chapters)],
        key=lambda x: chap_key_fn(x[0], x[1])
    )
    chapters_sorted = [c for c, _ in chapters_sorted]

    merged: List[Dict[str, Any]] = []
    n = len(chapters_sorted)
    bs = int(batch_size)

    for b_idx, start in enumerate(range(0, n, bs), start=1):
        group = chapters_sorted[start:start + bs]

        nums = []
        for c in group:
            if isinstance(c, dict):
                nums.append(_safe_int(c.get("Number", 0), 0))
        nums = [x for x in nums if x > 0]
        if nums:
            lo, hi = min(nums), max(nums)
            rng_str = f"{lo}-{hi}" if lo != hi else f"{lo}"
        else:
            rng_str = f"{start+1}-{min(start+bs, n)}"

        merged_title = f"Chapters {rng_str}"

        parts: List[str] = []
        for c in group:
            if not isinstance(c, dict):
                continue
            cnum = c.get("Number", "")
            ctext = str(c.get("text", "") or "")
            head = f"[Chapter {cnum}]".strip()
            if head:
                parts.append(head)
            if ctext:
                parts.append(ctext)

        merged_text = "\n\n".join(parts).strip()

        merged.append({
            "Number": int(b_idx),
            "title": merged_title,
            "text": merged_text,
        })

    target = book_record if inplace else dict(book_record)
    target[chap_key] = merged

    if chap_key == "chapter":
        target["chapters"] = merged
    else:
        target["chapter"] = merged

    target["_chapter_batched"] = True
    target["_chapter_batch_size"] = int(batch_size)
    target["_chapters_original_count"] = int(len(chapters))
    target["_chapters_batched_count"] = int(len(merged))
    target["_chap_key_used"] = chap_key
    return target


def make_run_id(
    method: str,
    use_persona: bool,
    use_discussion: bool,
    use_interest: bool,
    discussion_rounds: int,
    discussion_window: int,
    n_agents: int,
    score_decimals: int,
    discussion_affects_score: bool,
    chapter_batch_size: int,
) -> str:
    return sanitize_name(
        f"m={method}"
        f"__persona={int(use_persona)}"
        f"__disc={int(use_discussion)}"
        f"__interest={int(use_interest)}"
        f"__r={discussion_rounds}"
        f"__w={discussion_window}"
        f"__na={n_agents}"
        f"__sd={score_decimals}"
        f"__das={int(discussion_affects_score)}"
        f"__cbs={int(chapter_batch_size)}"
    )

def make_book_run_id(
    book_title: str,
    method: str,
    use_persona: bool,
    use_discussion: bool,
    use_interest: bool,
    discussion_rounds: int,
    discussion_window: int,
    n_agents: int,
    score_decimals: int,
    discussion_affects_score: bool,
    chapter_batch_size: int,
) -> str:
    book_tag = sanitize_name(book_title, max_len=80) or "UNKNOWN_BOOK"
    core = make_run_id(
        method=method,
        use_persona=use_persona,
        use_discussion=use_discussion,
        use_interest=use_interest,
        discussion_rounds=discussion_rounds,
        discussion_window=discussion_window,
        n_agents=n_agents,
        score_decimals=score_decimals,
        discussion_affects_score=discussion_affects_score,
        chapter_batch_size=chapter_batch_size,
    )
    # æœ€å¤–å±‚å† sanitize ä¸€æ¬¡ï¼Œä¿è¯æ•´ä½“å¯åšç›®å½•å
    return sanitize_name(f"book={book_tag}__{core}")

def build_cfg(
    output_root: Path,
    base_url: str,
    method: str,
    use_persona: bool,
    use_discussion: bool,
    use_interest: bool,
    discussion_rounds: int,
    discussion_window: int,
    n_agents: int,
    score_decimals: int,
    discussion_affects_score: bool,
    chapter_batch_size: int,
) -> Any:
    """
    æ„é€  runner éœ€è¦çš„ cfgï¼ˆå±æ€§è®¿é—®ï¼Œä¸ä¾èµ– yaml/CLIï¼‰
    """
    return SimpleNamespace(
        llm=SimpleNamespace(
            base_url=str(base_url),
            api_key_env=API_KEY_ENV,
            model=LLM_MODEL,
            temperature=LLM_TEMPERATURE,
            top_p=LLM_TOP_P,
            max_tokens=LLM_MAX_TOKENS,
            timeout_sec=LLM_TIMEOUT_SEC,

            retry_max_attempts=RETRY_MAX_ATTEMPTS,
            retry_base_sleep_sec=RETRY_BASE_SLEEP_SEC,
            retry_max_sleep_sec=RETRY_MAX_SLEEP_SEC,
            retry_jitter=RETRY_JITTER,
            fail_fast=FAIL_FAST,
        ),
        experiment=SimpleNamespace(
            method=method,
            use_persona=use_persona,
            use_discussion=use_discussion,
            use_interest_filter=use_interest,
            discussion_rounds=discussion_rounds,
            discussion_window=discussion_window,
            n_agents=n_agents,
            score_decimals=score_decimals,
            discussion_affects_score=discussion_affects_score,
            chapter_batch_size=int(chapter_batch_size),
        ),
        concurrency=SimpleNamespace(
            max_workers=PER_BOOK_AGENT_WORKERS
        ),
        paths=SimpleNamespace(
            output_root=str(output_root),
            base_eval_root=str(BASE_EVAL_ROOT),
        )
    )


def build_experiments_grid() -> List[Tuple]:
    exps: List[Tuple] = []
    for m in METHODS:
        for up in USE_PERSONA_OPTS:
            for ud in USE_DISCUSSION_OPTS:
                for ui in USE_INTEREST_FILTER_OPTS:
                    for rr in DISCUSSION_ROUNDS_OPTS:
                        for ww in DISCUSSION_WINDOW_OPTS:
                            for na in N_AGENTS_OPTS:
                                for sd in SCORE_DECIMALS_OPTS:
                                    for das in DISCUSSION_AFFECTS_SCORE_OPTS:
                                        exps.append((m, up, ud, ui, rr, ww, na, sd, das))
    return exps


# ============================================================
# æ ¸å¿ƒï¼šå®éªŒçº§â€œæ£€æŸ¥/è·³è¿‡/åˆ æ‰é‡è·‘â€
# ============================================================
def prepare_experiment_dir(exp_root: Path) -> Tuple[bool, Path, Path]:
    """
    è¿”å› (should_run, done_path, running_path)
    - should_run=False è¡¨ç¤ºå·²ç»åšè¿‡ï¼ˆDONE å­˜åœ¨ï¼‰ï¼Œç›´æ¥è·³è¿‡
    - should_run=True è¡¨ç¤ºè¦è·‘ï¼šè‹¥ç›®å½•å­˜åœ¨ä½†æ²¡ DONEï¼Œåˆ™åˆ æ‰é‡å»º
    """
    done_path = exp_root / "_DONE.json"
    running_path = exp_root / "_RUNNING.json"

    if done_path.exists():
        return False, done_path, running_path

    if exp_root.exists():
        shutil.rmtree(exp_root, ignore_errors=True)

    safe_mkdir(exp_root)
    safe_mkdir(exp_root / "logs")  # ç»™ runner å†™ jsonl/log ç”¨
    return True, done_path, running_path

# ============================================================
# å•æœ¬ä¹¦â€œå¤§ä»»åŠ¡â€ï¼šé¡ºåºè·‘æ‰€æœ‰å®éªŒé…ç½®ï¼ˆç»‘å®šä¸€ä¸ª base_urlï¼‰
# ============================================================
def run_all_experiments_for_book(
    book_index: int,
    base_url: str,
    books: List[Dict[str, Any]],
    personas_raw: List[Dict[str, Any]],
    experiments: List[Tuple],
    chapter_batch_size: int,
) -> Tuple[int, str, int, int]:
    """
    å¯¹å•æœ¬ä¹¦ï¼š
    - ç»‘å®šä¸€ä¸ªåƒé—®ç«¯ç‚¹ base_url
    - ä¾æ¬¡è·‘ experiments é‡Œçš„æ¯ä¸ªå®éªŒé…ç½®
    - æ¯ä¸ª (book, experiment_cfg) æ‹¥æœ‰ç‹¬ç«‹çš„ run_id / ç›®å½• / _DONE.json
    - è‹¥ _DONE.json å·²å­˜åœ¨ï¼šç›´æ¥è·³è¿‡
    è¿”å›ï¼š(book_index, book_title, n_ran, n_skipped)
    """
    book_record = books[book_index]
    meta = (book_record.get("metadata", {}) or {})
    book_title = str(meta.get("title", "UNKNOWN")).strip() or f"BOOK_{book_index+1}"

    print(f"[TASK] BOOK[{book_index+1}/{len(books)}] {book_title} | base_url={base_url} | exps={len(experiments)}")

    n_ran = 0
    n_skipped = 0

    # é¢„å…ˆåˆå¹¶ç« èŠ‚ï¼ˆé¿å…æ¯ä¸ªå®éªŒéƒ½å†åˆä¸€æ¬¡ï¼›inplace=Falseï¼Œä¸æ±¡æŸ“åŸ books åˆ—è¡¨ï¼‰
    processed_book = batch_merge_chapters(
        book_record,
        batch_size=int(chapter_batch_size),
        inplace=False,
    )

    for exp_local_idx, (m, up, ud, ui, rr, ww, na, sd, das) in enumerate(experiments):
        # é’ˆå¯¹ã€Œå•æœ¬ä¹¦ + å•å®éªŒé…ç½®ã€çš„ run_id
        run_id = make_book_run_id(
            book_title=book_title,
            method=m,
            use_persona=up,
            use_discussion=ud,
            use_interest=ui,
            discussion_rounds=rr,
            discussion_window=ww,
            n_agents=na,
            score_decimals=sd,
            discussion_affects_score=das,
            chapter_batch_size=chapter_batch_size,
        )

        exp_root = OUTPUTS_ROOT / run_id
        should_run, done_path, running_path = prepare_experiment_dir(exp_root)

        if not should_run:
            # è¿™ä¸ª (book, cfg) å·²ç»å®Œæˆï¼Œè·³è¿‡
            print(f"[SKIP EXP] book={book_title} | run_id={run_id} (DONE exists)")
            n_skipped += 1
            continue

        # ç¨å¾®é”™å³°ä¸€ä¸‹ï¼ˆåªå¯¹æ¯æœ¬ä¹¦çš„ç¬¬ä¸€ä¸ªå®éªŒåšå»¶è¿Ÿï¼‰
        if START_STAGGER_SEC > 0 and exp_local_idx == 0:
            time.sleep(book_index * START_STAGGER_SEC)

        logger = setup_logger(
            str(exp_root / "logs"),
            also_file=True,
            logger_name=f"llm_trace_{run_id}",
        )

        api_key = os.getenv(API_KEY_ENV, "")
        if not api_key:
            raise RuntimeError(f"ç¯å¢ƒå˜é‡ {API_KEY_ENV} æœªè®¾ç½®ã€‚")

        monitor = AdaptiveConcurrencyController(
            min_workers=1,
            max_workers=int(PER_BOOK_AGENT_WORKERS),
            latency_threshold_s=100.0,
            error_threshold=0.2,
        )

        llm = ThreadLocalLLM(
            base_url=str(base_url),
            api_key=api_key,
            model=LLM_MODEL,
            timeout_sec=int(LLM_TIMEOUT_SEC),
            logger=logger,
            retry_max_attempts=int(RETRY_MAX_ATTEMPTS),
            retry_base_sleep_sec=float(RETRY_BASE_SLEEP_SEC),
            retry_max_sleep_sec=float(RETRY_MAX_SLEEP_SEC),
            retry_jitter=float(RETRY_JITTER),
            fail_fast=bool(FAIL_FAST),
            monitor=monitor,
        )

        personas_used = personas_raw[: int(na)]

        atomic_write_json(
            running_path,
            {
                "started_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                "run_id": run_id,
                "book_index": book_index,
                "book_title": book_title,
                "base_url": str(base_url),
                "grid": {
                    "method": m,
                    "use_persona": up,
                    "use_discussion": ud,
                    "use_interest_filter": ui,
                    "discussion_rounds": rr,
                    "discussion_window": ww,
                    "n_agents": na,
                    "score_decimals": sd,
                    "discussion_affects_score": das,
                    "chapter_batch_size": int(chapter_batch_size),
                },
                "books": 1,
                "agents": len(personas_used),
            },
        )

        logger.info(
            "=== EXP START === run_id=%s | book=%s | base_url=%s | n_agents=%d | rounds=%d | window=%d | score_decimals=%d | das=%s | chapter_batch_size=%d",
            run_id,
            book_title,
            str(base_url),
            len(personas_used),
            rr,
            ww,
            sd,
            str(das),
            int(chapter_batch_size),
        )

        # æ„é€  cfgï¼ˆæ³¨æ„ output_root æ¢æˆäº† exp_rootï¼‰
        cfg = build_cfg(
            output_root=exp_root,
            base_url=base_url,
            method=m,
            use_persona=up,
            use_discussion=ud,
            use_interest=ui,
            discussion_rounds=rr,
            discussion_window=ww,
            n_agents=na,
            score_decimals=sd,
            discussion_affects_score=das,
            chapter_batch_size=chapter_batch_size,
        )

        # ğŸ”‘ è¿™é‡Œåªè¯„æµ‹ã€Œè¿™ä¸€æœ¬æ–‡ä¹¦ã€â€”â€”ä¸å†åœ¨è¿™é‡Œ for æ‰€æœ‰ books
        evaluate_single_book(cfg, llm, logger, processed_book, personas_used)

        atomic_write_json(
            done_path,
            {
                "finished_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                "run_id": run_id,
                "book_index": book_index,
                "book_title": book_title,
            },
        )
        try:
            if running_path.exists():
                running_path.unlink()
        except Exception:
            pass

        logger.info("=== EXP END === run_id=%s | book=%s", run_id, book_title)
        n_ran += 1

    return book_index, book_title, n_ran, n_skipped

# ============================================================
# å•ä¸ªå®éªŒï¼šé¡ºåºè·‘æ‰€æœ‰ä¹¦ï¼ˆæ¯ä¸ªå¤§å®éªŒç»‘å®šä¸€ä¸ª base_urlï¼‰
# ============================================================
def run_one_experiment(
    exp_index: int,
    base_url: str,
    books: List[Dict[str, Any]],
    personas_raw: List[Dict[str, Any]],
    method: str,
    use_persona: bool,
    use_discussion: bool,
    use_interest: bool,
    discussion_rounds: int,
    discussion_window: int,
    n_agents: int,
    score_decimals: int,
    discussion_affects_score: bool,
    chapter_batch_size: int,
) -> str:
    run_id = make_run_id(
        method, use_persona, use_discussion, use_interest,
        discussion_rounds, discussion_window, n_agents,
        score_decimals, discussion_affects_score,
        chapter_batch_size=chapter_batch_size,
    )

    exp_root = OUTPUTS_ROOT / run_id

    should_run, done_path, running_path = prepare_experiment_dir(exp_root)
    if not should_run:
        print(f"[SKIP EXP] {run_id} (DONE exists)")
        return run_id

    if START_STAGGER_SEC > 0:
        time.sleep(exp_index * START_STAGGER_SEC)

    logger = setup_logger(str(exp_root / "logs"), also_file=True, logger_name=f"llm_trace_{run_id}")

    api_key = os.getenv(API_KEY_ENV, "")
    if not api_key:
        raise RuntimeError(f"ç¯å¢ƒå˜é‡ {API_KEY_ENV} æœªè®¾ç½®ã€‚")

    # è‡ªé€‚åº”å¹¶å‘æ§åˆ¶å™¨ï¼šrunner å†…éƒ¨æŒ‰é˜¶æ®µä½¿ç”¨ suggest_workers è°ƒæ•´æ¯ä¸ªé˜¶æ®µçš„ ThreadPoolExecutor å¤§å°
    monitor = AdaptiveConcurrencyController(
        min_workers=1,
        max_workers=int(PER_BOOK_AGENT_WORKERS),
        latency_threshold_s=100.0,
        error_threshold=0.2,
    )

    llm = ThreadLocalLLM(
        base_url=str(base_url),
        api_key=api_key,
        model=LLM_MODEL,
        timeout_sec=int(LLM_TIMEOUT_SEC),
        logger=logger,
        retry_max_attempts=int(RETRY_MAX_ATTEMPTS),
        retry_base_sleep_sec=float(RETRY_BASE_SLEEP_SEC),
        retry_max_sleep_sec=float(RETRY_MAX_SLEEP_SEC),
        retry_jitter=float(RETRY_JITTER),
        fail_fast=bool(FAIL_FAST),
        monitor=monitor,
    )

    personas_used = personas_raw[: int(n_agents)]

    atomic_write_json(running_path, {
        "started_at": time.strftime("%Y-%m-%d %H:%M:%S"),
        "run_id": run_id,
        "base_url": str(base_url),
        "grid": {
            "method": method,
            "use_persona": use_persona,
            "use_discussion": use_discussion,
            "use_interest_filter": use_interest,
            "discussion_rounds": discussion_rounds,
            "discussion_window": discussion_window,
            "n_agents": n_agents,
            "score_decimals": score_decimals,
            "discussion_affects_score": discussion_affects_score,
            "chapter_batch_size": int(chapter_batch_size),
        },
        "books": len(books),
        "agents": len(personas_used),
    })

    logger.info(
        "=== EXP START === run_id=%s | base_url=%s | books=%d | n_agents=%d | rounds=%d | window=%d | score_decimals=%d | das=%s | chapter_batch_size=%d",
        run_id, str(base_url), len(books), len(personas_used), discussion_rounds, discussion_window, score_decimals,
        str(discussion_affects_score), int(chapter_batch_size)
    )

    cfg = build_cfg(
        output_root=exp_root,
        base_url=base_url,
        method=method,
        use_persona=use_persona,
        use_discussion=use_discussion,
        use_interest=use_interest,
        discussion_rounds=discussion_rounds,
        discussion_window=discussion_window,
        n_agents=n_agents,
        score_decimals=score_decimals,
        discussion_affects_score=discussion_affects_score,
        chapter_batch_size=chapter_batch_size,
    )

    for i, book_record in enumerate(books, start=1):
        title = (book_record.get("metadata", {}) or {}).get("title", "UNKNOWN")

        processed_book = batch_merge_chapters(book_record, batch_size=int(chapter_batch_size))

        orig_cnt = processed_book.get("_chapters_original_count", None)
        bat_cnt = processed_book.get("_chapters_batched_count", None)
        if orig_cnt is not None and bat_cnt is not None and int(chapter_batch_size) > 1:
            logger.info("RUN BOOK [%d/%d] %s | chapters %s -> %s (batch_size=%d)",
                        i, len(books), title, orig_cnt, bat_cnt, int(chapter_batch_size))
        else:
            logger.info("RUN BOOK [%d/%d] %s", i, len(books), title)

        evaluate_single_book(cfg, llm, logger, processed_book, personas_used)

    atomic_write_json(done_path, {
        "finished_at": time.strftime("%Y-%m-%d %H:%M:%S"),
        "run_id": run_id,
    })
    try:
        if running_path.exists():
            running_path.unlink()
    except Exception:
        pass

    logger.info("=== EXP END === run_id=%s", run_id)
    return run_id


def main() -> None:
    safe_mkdir(BATCH_ROOT)
    safe_mkdir(OUTPUTS_ROOT)
    safe_mkdir(BASE_EVAL_ROOT)

    books = read_json(BOOKS_MERGED_JSON)
    personas_raw = read_json(PERSONAS_JSON)
    experiments = build_experiments_grid()

    n_books = len(books)
    n_exps = len(experiments)

    print(f"[BATCH] batch_root={BATCH_ROOT}")
    print(f"[BATCH] outputs_root={OUTPUTS_ROOT}")
    print(f"[BATCH] base_eval_root={BASE_EVAL_ROOT}")
    print(f"[BATCH] books={n_books} personas={len(personas_raw)}")
    print(f"[BATCH] experiments_per_book={n_exps}")
    print(f"[BATCH] chapter_batch_size={CHAPTER_BATCH_SIZE}")

    n_endpoints = max(1, len(QWEN_BASE_URLS))
    max_workers = min(MAX_EXPERIMENT_WORKERS, n_endpoints, n_books)

    print(f"[BATCH] qwen_endpoints={n_endpoints} max_workers={max_workers}")

    # é¡¶å±‚å¹¶å‘ï¼šæ¯ä¸ª worker æŒç»­å¤„ç†è‹¥å¹²æœ¬ä¹¦
    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        futs = []
        for book_idx in range(n_books):
            base_url = QWEN_BASE_URLS[book_idx % n_endpoints]
            futs.append(
                ex.submit(
                    run_all_experiments_for_book,
                    book_idx,
                    base_url,
                    books,
                    personas_raw,
                    experiments,
                    CHAPTER_BATCH_SIZE,
                )
            )

        done_books = 0
        for fu in as_completed(futs):
            book_idx, book_title, n_ran, n_skipped = fu.result()
            done_books += 1
            print(
                f"[{done_books}/{n_books}] BOOK DONE {book_title} | exps_ran={n_ran} | exps_skipped={n_skipped}"
            )

    print(f"[BATCH] ALL BOOKS DONE. Raw outputs under: {OUTPUTS_ROOT}")
    print(f"[BATCH] Base eval cache under: {BASE_EVAL_ROOT}")



def test() -> None:
    """
    çº¯é¡ºåºè°ƒè¯•ç‰ˆï¼ˆä¸å¹¶å‘ï¼‰ï¼Œæ–¹ä¾¿æœ¬åœ°å•æ­¥è°ƒè¯•ã€‚
    """
    safe_mkdir(BATCH_ROOT)
    safe_mkdir(OUTPUTS_ROOT)
    safe_mkdir(BASE_EVAL_ROOT)

    books = read_json(BOOKS_MERGED_JSON)
    personas_raw = read_json(PERSONAS_JSON)
    experiments = build_experiments_grid()

    n_books = len(books)
    n_exps = len(experiments)

    print(f"[BATCH-TEST] batch_root={BATCH_ROOT}")
    print(f"[BATCH-TEST] outputs_root={OUTPUTS_ROOT}")
    print(f"[BATCH-TEST] base_eval_root={BASE_EVAL_ROOT}")
    print(f"[BATCH-TEST] books={n_books} personas={len(personas_raw)}")
    print(f"[BATCH-TEST] experiments_per_book={n_exps} (SEQUENTIAL)")
    print(f"[BATCH-TEST] chapter_batch_size={CHAPTER_BATCH_SIZE}")

    base_url = QWEN_BASE_URLS[0] if QWEN_BASE_URLS else "http://localhost:8000/v1"

    done_books = 0
    for book_idx in range(n_books):
        _, book_title, n_ran, n_skipped = run_all_experiments_for_book(
            book_index=book_idx,
            base_url=base_url,
            books=books,
            personas_raw=personas_raw,
            experiments=experiments,
            chapter_batch_size=CHAPTER_BATCH_SIZE,
        )
        done_books += 1
        print(
            f"[{done_books}/{n_books}] BOOK DONE {book_title} | exps_ran={n_ran} | exps_skipped={n_skipped}"
        )

    print(f"[BATCH-TEST] DONE. Raw outputs under: {OUTPUTS_ROOT}")
    print(f"[BATCH-TEST] Base eval cache under: {BASE_EVAL_ROOT}")



if __name__ == "__main__":
    main()
    # test()
